{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mwparserfromhell in /mnt/ssd-1/lucia/miniconda3/envs/3.10/lib/python3.10/site-packages (0.6.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run block in tmux\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from chunk import chunk_and_tokenize\n",
    "\n",
    "output_dir = Path('/') / 'mnt' / 'ssd-1' / 'lucia'\n",
    "\n",
    "# dataset = load_dataset(\n",
    "#     \"allenai/c4\", \n",
    "#     \"es\", \n",
    "#     split='train', \n",
    "#     download_config=DownloadConfig(resume_download=True),\n",
    "#     cache_dir=f\"/mnt/ssd-1/hf_cache\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"spanish_billion_words\", \n",
    "    split='train', \n",
    "    download_config=DownloadConfig(resume_download=True),\n",
    "    cache_dir=Path('/') / 'mnt' / 'ssd-1' / 'hf_cache')\n",
    "\n",
    "model_name = \"pythia-160m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"EleutherAI/{model_name}\")   \n",
    "# filtered_dataset = dataset.shuffle().select(range(int(3.5e11)))\n",
    "shuffled = dataset.shuffle()\n",
    "data, bpb_ratio = chunk_and_tokenize(shuffled, tokenizer, max_length=2049)\n",
    "\n",
    "data.save_to_disk(output_dir / 'es_1b_full_tokenized.hf')\n",
    "\n",
    "fp = np.memmap(output_dir / 'es_1b_full.bin', dtype=np.uint16, mode='w+', shape=(len(data), 2049))\n",
    "for i, item in enumerate(data):\n",
    "    fp[i] = item['input_ids'].numpy()   \n",
    "fp.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es = load_dataset(\"allenai/c4\", \"es\", split='train')\n",
    "es = load_dataset(\"es/pile-10k\", split='train')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "data, bpb_ratio = chunk_and_tokenize(es, tokenizer, max_length=2049)\n",
    "data.save_to_disk(\"es_tokenized.hf\")\n",
    "\n",
    "fp = np.memmap('es.bin', dtype=np.uint16, mode='w+', shape=(len(data), 2049))\n",
    "for i, item in enumerate(data):\n",
    "    fp[i] = item['input_ids'].numpy()\n",
    "fp.flush()\n",
    "\n",
    "print(len(data))\n",
    "print(data[\"length\"][0:6])\n",
    "print(len(data[\"input_ids\"][0]))\n",
    "print(sum(data[\"length\"]))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first = 0\n",
    "# sec = 0\n",
    "# third = 0\n",
    "# four = 0\n",
    "# for item in data['length']:\n",
    "#     if item == 2048:\n",
    "#         first += 1\n",
    "#     elif item == 2049:\n",
    "#         sec += 1\n",
    "#     elif item == 2050:\n",
    "#         third += 1\n",
    "#     elif item == 2051:\n",
    "#         four += 1\n",
    "#     else:\n",
    "#         print(item)\n",
    "\n",
    "# print(first, sec, third, four)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
